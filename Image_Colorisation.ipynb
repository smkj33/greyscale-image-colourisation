{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Colorisation",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "nryzBDJeNJKB"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smkj33/greyscale-image-colourisation/blob/master/Image_Colorisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRbpdjn-MnSU",
        "colab_type": "text"
      },
      "source": [
        "# Imports\n",
        "**Try** to keep Most of the imports in this cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhE0qq34BP5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from skimage import color\n",
        "from skimage.transform import resize\n",
        "import scipy.ndimage.interpolation as sni\n",
        "from skimage import color, io\n",
        "\n",
        "# import cv2\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Pytorch device: \" + str(device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5tYoGvpMsx-",
        "colab_type": "text"
      },
      "source": [
        "# RUNTIME ENVIRONMENT\n",
        "Loaded all codes into google colab - drive used: sid.mkjee.h5@gmail.com;   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcRQS7ZZBKXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this saves the model to your own Google Drive\n",
        "# once you run, you will have to follow a generated link and get the authorization code\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nryzBDJeNJKB",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTING DATASET (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewq6GfzprqPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Using the following commands, python connects to kaggle and\n",
        "the data set is downloaded in the colab environment.\n",
        "This will get deleted as soon as the instance is disconnected.\n",
        "To avoid this the data is permanently uploaded in my google drive\n",
        "\"\"\"\n",
        "\n",
        "# Using my Kaggle Token to import dataset.  \n",
        "\n",
        "# !mkdir ~/.kaggle\n",
        "# !touch ~/.kaggle/kaggle.json\n",
        "# api_token = {\"username\":\"smukherj\",\"key\":\"<---add latest key here--->\"}\n",
        "\n",
        "# import json\n",
        "# with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "#     json.dump(api_token, file)\n",
        "\n",
        "# !chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1ns_5DXz6oK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "The following dataset is used:  \n",
        "https://www.kaggle.com/shravankumar9892/image-colorization#gray_scale.npy   \n",
        "\"\"\"\n",
        "\n",
        "# !kaggle datasets download -d shravankumar9892/image-colorization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYSpQELZBtIc",
        "colab_type": "text"
      },
      "source": [
        "# DATA PREPROCESSING \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2mLmGE9BtQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "File is obtained in zip format and uploaded to drive.  \n",
        "Python is used for unzipping.\n",
        "\n",
        "DO NOT RERUN, FILE ALREADY EXTRACTED!!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# import zipfile\n",
        "# with zipfile.ZipFile('image-colorization.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall(path = 'gdrive/My Drive/Datasets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XqKECv2TUiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference: https://stackoverflow.com/questions/57989716/loading-npy-files-as-dataset-for-pytorch\n",
        "# https://discuss.pytorch.org/t/loading-npy-files-using-torchvision/28481/2\n",
        "# Imports npy files into tensor data structure\n",
        "def npy_loader(path, file_name):\n",
        "    return torch.from_numpy(np.load(path + file_name))\n",
        "\n",
        "def reconstruct_predicted(image_gray, image_ab):\n",
        "    '''Reconstruct image from l+ab, convert to rgb uint8'''\n",
        "    image_gray = image_gray.squeeze(0).squeeze(0)\n",
        "    image_ab = image_ab.squeeze(0).permute((1,2,0))\n",
        "    img = np.zeros((256, 256, 3))\n",
        "    img[:, :, 0] = image_gray\n",
        "    img[:, :, 1:] = image_ab\n",
        "    img = color.lab2rgb(img)  # (256, 256, 3), float(0., 1.)\n",
        "    img = img * 255\n",
        "    img = img.astype('uint8')  # (256, 256, 3), int(0, 255)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgmbVKNCSZ0B",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EP5IT2yScZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "minibatch_size = 1\n",
        "n_epochs = 100\n",
        "# TODO: add other hyperparams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c54d7mfrSXqV",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC4vm0LxTbPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Dataset\n",
        "path = '/content/gdrive/My Drive/Datasets/Archive/ab/ab/'\n",
        "abval1 = npy_loader(path, 'ab1.npy')\n",
        "abval2 = npy_loader(path, 'ab2.npy')\n",
        "abval3 = npy_loader(path, 'ab3.npy')\n",
        "\n",
        "path =  '/content/gdrive/My Drive/Datasets/Archive/l/'\n",
        "lval = npy_loader(path, 'gray_scale.npy')\n",
        "\n",
        "ab_temp = torch.cat((abval1, abval2), 0)\n",
        "abval = torch.cat((ab_temp , abval3), 0)\n",
        "\n",
        "# split into training and testing\n",
        "lval_training, lval_testing = lval.split((lval.shape[0] - 5000, 5000))\n",
        "abval_training, abval_testing = abval.split((lval.shape[0] - 5000, 5000))\n",
        "\n",
        "# split to mini_batch sized chunks\n",
        "lval_training = torch.split(lval_training, minibatch_size, 0)\n",
        "lval_testing = torch.split(lval_testing, minibatch_size, 0)\n",
        "abval_training = torch.split(abval_training, minibatch_size, 0)\n",
        "abval_testing = torch.split(abval_testing, minibatch_size, 0)\n",
        "n_training = len(lval_training)\n",
        "n_testing = len(lval_testing)\n",
        "print(\"Number training batches: {}\\nNumber testing batches: {}\".format(n_training, n_testing))\n",
        "print(\"Shape of a batch for l: {}\".format(lval_training[0].shape))\n",
        "print(\"Shape of a batch for ab: {}\".format(abval_training[0].shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCY_83gDoe8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bins for quantized ab color space\n",
        "points = torch.tensor([[-90, 50], [-90, 60], [-90, 70], [-90, 80], [-90, 90], [-80, 20], [-80, 30], [-80, 40], [-80, 50], [-80, 60], [-80, 70], [-80, 80], [-80, 90], [-70, 0], [-70, 10], [-70, 20], [-70, 30], [-70, 40], [-70, 50], [-70, 60], [-70, 70], [-70, 80], [-70, 90], [-60, -20], [-60, -10], [-60, 0], [-60, 10], [-60, 20], [-60, 30], [-60, 40], [-60, 50], [-60, 60], [-60, 70], [-60, 80], [-60, 90], [-50, -30], [-50, -20], [-50, -10], [-50, 0], [-50, 10], [-50, 20], [-50, 30], [-50, 40], [-50, 50], [-50, 60], [-50, 70], [-50, 80], [-50, 90], [-50, 100], [-40, -40], [-40, -30], [-40, -20], [-40, -10], [-40, 0], [-40, 10], [-40, 20], [-40, 30], [-40, 40], [-40, 50], [-40, 60], [-40, 70], [-40, 80], [-40, 90], [-40, 100], [-30, -50], [-30, -40], [-30, -30], [-30, -20], [-30, -10], [-30, 0], [-30, 10], [-30, 20], [-30, 30], [-30, 40], [-30, 50], [-30, 60], [-30, 70], [-30, 80], [-30, 90], [-30, 100], [-20, -50], [-20, -40], [-20, -30], [-20, -20], [-20, -10], [-20, 0], [-20, 10], [-20, 20], [-20, 30], [-20, 40], [-20, 50], [-20, 60], [-20, 70], [-20, 80], [-20, 90], [-20, 100], [-10, -60], [-10, -50], [-10, -40], [-10, -30], [-10, -20], [-10, -10], [-10, 0], [-10, 10], [-10, 20], [-10, 30], [-10, 40], [-10, 50], [-10, 60], [-10, 70], [-10, 80], [-10, 90], [-10, 100], [0, -70], [0, -60], [0, -50], [0, -40], [0, -30], [0, -20], [0, -10], [0, 0], [0, 10], [0, 20], [0, 30], [0, 40], [0, 50], [0, 60], [0, 70], [0, 80], [0, 90], [0, 100], [10, -80], [10, -70], [10, -60], [10, -50], [10, -40], [10, -30], [10, -20], [10, -10], [10, 0], [10, 10], [10, 20], [10, 30], [10, 40], [10, 50], [10, 60], [10, 70], [10, 80], [10, 90], [20, -80], [20, -70], [20, -60], [20, -50], [20, -40], [20, -30], [20, -20], [20, -10], [20, 0], [20, 10], [20, 20], [20, 30], [20, 40], [20, 50], [20, 60], [20, 70], [20, 80], [20, 90], [30, -90], [30, -80], [30, -70], [30, -60], [30, -50], [30, -40], [30, -30], [30, -20], [30, -10], [30, 0], [30, 10], [30, 20], [30, 30], [30, 40], [30, 50], [30, 60], [30, 70], [30, 80], [30, 90], [40, -100], [40, -90], [40, -80], [40, -70], [40, -60], [40, -50], [40, -40], [40, -30], [40, -20], [40, -10], [40, 0], [40, 10], [40, 20], [40, 30], [40, 40], [40, 50], [40, 60], [40, 70], [40, 80], [40, 90], [50, -100], [50, -90], [50, -80], [50, -70], [50, -60], [50, -50], [50, -40], [50, -30], [50, -20], [50, -10], [50, 0], [50, 10], [50, 20], [50, 30], [50, 40], [50, 50], [50, 60], [50, 70], [50, 80], [60, -110], [60, -100], [60, -90], [60, -80], [60, -70], [60, -60], [60, -50], [60, -40], [60, -30], [60, -20], [60, -10], [60, 0], [60, 10], [60, 20], [60, 30], [60, 40], [60, 50], [60, 60], [60, 70], [60, 80], [70, -110], [70, -100], [70, -90], [70, -80], [70, -70], [70, -60], [70, -50], [70, -40], [70, -30], [70, -20], [70, -10], [70, 0], [70, 10], [70, 20], [70, 30], [70, 40], [70, 50], [70, 60], [70, 70], [70, 80], [80, -110], [80, -100], [80, -90], [80, -80], [80, -70], [80, -60], [80, -50], [80, -40], [80, -30], [80, -20], [80, -10], [80, 0], [80, 10], [80, 20], [80, 30], [80, 40], [80, 50], [80, 60], [80, 70], [90, -110], [90, -100], [90, -90], [90, -80], [90, -70], [90, -60], [90, -50], [90, -40], [90, -30], [90, -20], [90, -10], [90, 0], [90, 10], [90, 20], [90, 30], [90, 40], [90, 50], [90, 60], [90, 70], [100, -90], [100, -80], [100, -70], [100, -60], [100, -50], [100, -40], [100, -30], [100, -20], [100, -10], [100, 0]]).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9W6W5dRe1UH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDatasetLAB():\n",
        "    def __init__(self, lval, abval):\n",
        "        # array of luminances and chrominances\n",
        "        self.l = lval\n",
        "        self.ab = abval\n",
        "\n",
        "    def size(self):\n",
        "        return len(lval)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_l = self.l[idx]\n",
        "        img_ab = self.ab[idx]\n",
        "        img_l = resize(img_l, (minibatch_size, 256, 256), preserve_range=True) #TODO: rewrite with F.interpolate\n",
        "        img_ab = resize(img_ab, (minibatch_size, 256, 256), preserve_range=True)  #TODO: rewrite with F.interpolate\n",
        "\n",
        "        img_l = torch.from_numpy(img_l).unsqueeze(1).float().to(device)\n",
        "        # print(img_ab.shape) \n",
        "\n",
        "        img_ab = torch.from_numpy(img_ab).permute(0,3,1,2).float().to(device)\n",
        "        # print(img_l.shape)  # (32, 1, 256, 256)\n",
        "        # print(img_ab.shape)  # (32, 2, 256, 256)\n",
        "\n",
        "        img_l = img_l / 256 * 110\n",
        "        img_ab = img_ab - 128\n",
        "        # print(img_ab.shape)\n",
        "        return (img_l, img_ab)\n",
        "\n",
        "training_dataset = CustomDatasetLAB(lval_training, abval_training)\n",
        "testing_dataset = CustomDatasetLAB(lval_testing, abval_testing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2cKvquQjilL",
        "colab_type": "text"
      },
      "source": [
        "# Preview dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWVAtM2Sjl40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l, ab = testing_dataset[1]  # batch\n",
        "n = 0 # image in batch\n",
        "l = l[n:n+1]\n",
        "ab = ab[n:n+1]\n",
        "img = reconstruct_predicted(l.to('cpu'), ab.to('cpu'))\n",
        "plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d4yFlBI1ZOJ",
        "colab_type": "text"
      },
      "source": [
        "# Colorizer NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SkqRtB8mcsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    ''' Net parameters as in: https://github.com/richzhang/colorization/blob/master/colorization/models/colorization_deploy_v2.prototxt '''\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(1, 64, 3, padding=1).to(device)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, 3, stride=2, padding=1).to(device)\n",
        "        self.bn1_2 = nn.BatchNorm2d(64)  # TODO: what does batchnorm do?\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1).to(device)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, 3, stride=2, padding=1).to(device)\n",
        "        self.bn2_2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1).to(device)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1).to(device)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, 3, 2, padding=1).to(device)\n",
        "        self.bn3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1).to(device)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1).to(device)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1).to(device)\n",
        "        self.bn4_3 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=2, dilation=2).to(device)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=2, dilation=2).to(device)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=2, dilation=2).to(device)\n",
        "        self.bn5_3 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv6_1 = nn.Conv2d(512, 512, 3, padding=2, dilation=2).to(device)\n",
        "        self.conv6_2 = nn.Conv2d(512, 512, 3, padding=2, dilation=2).to(device)\n",
        "        self.conv6_3 = nn.Conv2d(512, 512, 3, padding=2, dilation=2).to(device)\n",
        "        self.bn6_3 = nn.BatchNorm2d(512)\n",
        "       \n",
        "        self.conv7_1 = nn.Conv2d(512, 512, 3, padding=1, dilation=1).to(device)\n",
        "        self.conv7_2 = nn.Conv2d(512, 512, 3, padding=1, dilation=1).to(device)\n",
        "        self.conv7_3 = nn.Conv2d(512, 512, 3, padding=1, dilation=1).to(device)\n",
        "        self.bn7_3 = nn.BatchNorm2d(512)\n",
        "\n",
        "        # TODO: what is deconvolution?\n",
        "        self.conv8_1 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1, dilation=1).to(device)\n",
        "        self.conv8_2 = nn.Conv2d(256, 256, 3, padding=1, dilation=1).to(device)\n",
        "        self.conv8_3 = nn.Conv2d(256, 256, 3, padding=1, dilation=1).to(device)\n",
        "        \n",
        "        self.conv8_313 = nn.Conv2d(256, 313, kernel_size=1).to(device)\n",
        "        self.softmax = nn.Softmax(dim=1).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' expects l tensor[1,1,256,256] '''\n",
        "        x = self.bn1_2(F.relu(self.conv1_2(F.relu(self.conv1_1(x)))))\n",
        "        x = self.bn2_2(F.relu(self.conv2_2(F.relu(self.conv2_1(x)))))\n",
        "        x = self.bn3_3(F.relu(self.conv3_3(F.relu(self.conv3_2(F.relu(self.conv3_1(x)))))))\n",
        "        x = self.bn4_3(F.relu(self.conv4_3(F.relu(self.conv4_2(F.relu(self.conv4_1(x)))))))\n",
        "        x = self.bn5_3(F.relu(self.conv5_3(F.relu(self.conv5_2(F.relu(self.conv5_1(x)))))))\n",
        "        x = self.bn6_3(F.relu(self.conv6_3(F.relu(self.conv6_2(F.relu(self.conv6_1(x)))))))\n",
        "        x = self.bn7_3(F.relu(self.conv7_3(F.relu(self.conv7_2(F.relu(self.conv7_1(x)))))))\n",
        "        x = F.relu(self.conv8_3(F.relu(self.conv8_2(F.relu(self.conv8_1(x))))))\n",
        "        z_hat = self.softmax(self.conv8_313(x))\n",
        "        return z_hat\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CDGCX3CX-pL9"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P085atbrRohE",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def get_index(ab):\n",
        "    ''' Quantize image and return bin-number for whole h*w\n",
        "    @param: in_data - np.array in shape [1,2,h,w]\n",
        "    @return: idx_bin - np.array in shape [1,h,w]\n",
        "     '''\n",
        "    ab = ab.unsqueeze(1) # make dimension for z broadcasting (1,1,2,64,64)\n",
        "    points_reshaped = points.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)  # (1, 313, 2, 1, 1)\n",
        "    distance = (ab - points_reshaped)**2 # (1, 313, 2, 64, 64)\n",
        "    avg_distance = torch.mean(distance, dim = 2) # (1, 313, 64, 64) n.b. closest bin is (a_dist + b_dist)/2\n",
        "    bin_idx = avg_distance.argmin(dim = 1)  # (1, 64, 64)\n",
        "    # print(bin_idx.shape)\n",
        "    return bin_idx\n",
        "\n",
        "# NOTE: This is the one-hot encoded version\n",
        "def h_inverse(ab):\n",
        "    ''' construct z tensor given ab ground truth. Hard encoded.\n",
        "    @param ab, tensor([224,224,2])\n",
        "    @return z, tensor[1,64,64,313]\n",
        "    '''\n",
        "    ab_idx = get_index(ab)\n",
        "    z = F.one_hot(ab_idx, 313).float()\n",
        "    return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHeoudOx1fDB",
        "colab_type": "text"
      },
      "source": [
        "# Init Net, Loss, Optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt2yzaP5YdZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net()\n",
        "net.to(device)\n",
        "\n",
        "class OutCrossEntropyLoss(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(OutCrossEntropyLoss,self).__init__()\n",
        "        \n",
        "    def forward(self,z,z_hat):\n",
        "        mul = z.mul(z_hat.log2())\n",
        "        # print(mul)\n",
        "        # print(mul.sum(1))\n",
        "        MSE = mul.sum()\n",
        "        return -MSE\n",
        "\n",
        "# criterion = nn.MSELoss()  # Simple L2 squared loss\n",
        "criterion = OutCrossEntropyLoss()  # Cross Entropy Loss\n",
        "criterion.to(device)\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "# optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.99), weight_decay=0.004)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFyr7xTc2YdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.set_printoptions(edgeitems=10)  # widen print output\n",
        "losses = []\n",
        "\n",
        "class JobDone( Exception ):\n",
        "    pass\n",
        "try:\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "        # dummy_image = training_dataset[0]\n",
        "        for i, data in enumerate(training_dataset, 0):\n",
        "        # for i in range(100000):\n",
        "            # l, ab = dummy_image        \n",
        "            l, ab = data       \n",
        "            # print(np.unique(get_index(ab_64).flatten()).tolist()) # DEBUG: all unique colors in the image\n",
        "            ab_64 = F.interpolate(ab, (64,64))\n",
        "            # given ground truth ab, convert it to probabilities. Paper page 5, footnote\n",
        "            z = h_inverse(ab_64)  # (minibatch_size, 64, 64, 313)\n",
        "            z = z.permute(0,3,1,2)  # (minibatch_size, 313, 64, 64)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            z_hat = net(l)\n",
        "\n",
        "            loss = criterion(z, z_hat)  # cross entropy loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), 0.5)  # TODO: useful?\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            losses.append(loss.item())\n",
        "            if i % 1000 == 0:\n",
        "                print('Epoch {}/{}, minibatch {}/{}(size = {}): loss {}'.format(epoch, n_epochs, i, n_training, minibatch_size, loss.item()))\n",
        "                # running_loss = 0.0\n",
        "\n",
        "                # Save colorization preview for first image of testing_dataset\n",
        "                l, ab = testing_dataset[12] # minibatch 0\n",
        "                l = l[0:1] # image 0\n",
        "                ab = ab[0:1] # image 0\n",
        "                z_hat = net(l)\n",
        "                ab_hat = H(z_hat)\n",
        "                ab_hat = ab_hat.permute(2,0,1).unsqueeze(0)\n",
        "                ab_hat_256 = F.interpolate(ab_hat, (256, 256))\n",
        "                img_rgb = reconstruct_predicted(l.cpu(), ab_hat_256.detach().cpu())\n",
        "                PATH = '/content/gdrive/My Drive/SavedModels/Results/colorized{}_{}.png'.format(epoch, i)\n",
        "                img = Image.fromarray(img_rgb)\n",
        "                img.save(PATH)\n",
        "                print(\"       Colorization preview saved\")\n",
        "\n",
        "            if i % 20000 == 0:\n",
        "                # Save model checkpoints\n",
        "                PATH = '/content/gdrive/My Drive/SavedModels/Checkpoints/dummy_image_{}_{}.pth'.format(epoch,i)\n",
        "                torch.save(net.state_dict(), PATH)\n",
        "                print(\"       Checkpoint saved.\")\n",
        "            if (torch.isnan(z_hat[0,0,0,0])):\n",
        "                raise ValueError(\"ERROR: One of the predicted values is NaN, aborting.\")\n",
        "            if loss.item() < 500:\n",
        "                raise JobDone\n",
        "except JobDone:   \n",
        "    print('Finished Training')\n",
        "    PATH = '/content/gdrive/My Drive/SavedModels/Checkpoints/job_done.pth'.format(epoch,i)\n",
        "    torch.save(net.state_dict(), PATH)\n",
        "    print(\"       Job saved.\")\n",
        "else:\n",
        "    print(\"Job failed due to NaN error\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t93S6RVCxSEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display statistics of loss\n",
        "# print(running_losses)\n",
        "fig = plt.figure()\n",
        "running_loss = 0\n",
        "running_losses = []\n",
        "n = 16  # running mean window\n",
        "for i, loss in enumerate(losses):\n",
        "    running_loss += loss\n",
        "    if i % n == 0 and i != 0:\n",
        "        running_losses.append(running_loss / n)\n",
        "        running_loss = 0\n",
        "plt.plot(running_losses)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMZyp82dAs8f",
        "colab_type": "text"
      },
      "source": [
        "# Saving a trained model (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcEQRKCJAwz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = '/content/gdrive/My Drive/SavedModels/colorization_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "print(\"Saved.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t5OOssVA7yv",
        "colab_type": "text"
      },
      "source": [
        "# Loading a trained model (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBOIharDA_Vu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Net()\n",
        "PATH = '/content/gdrive/My Drive/SavedModels/Checkpoints/dummy_image_8_0.pth'\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "net.to(device)\n",
        "print(\"Loaded.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv8UtoVpBMro",
        "colab_type": "text"
      },
      "source": [
        "#Displaying trained Colorizer results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQkJdwtMBQPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def H(z_hat):\n",
        "    \"\"\"\n",
        "    Input: Z_hat (1,313,64,64)\n",
        "     - 313 probablities for each of the H * W pixels.\n",
        "      This is for 313 quantised a-b bins\n",
        "\n",
        "    Output: Y_hat (64, 64, 2)\n",
        "    - ab value matrix\n",
        "    \"\"\"\n",
        "    z_hat_softmax = f_t(z_hat)\n",
        "    z_hat_softmax = z_hat_softmax.squeeze(0).permute(1,2,0) # remove singleton dimension 0\n",
        "    # print(z_hat_softmax.shape) # [64,64,313]\n",
        "    a_bins = points[:,0].unsqueeze(0).unsqueeze(0)\n",
        "    b_bins = points[:,1].unsqueeze(0).unsqueeze(0)\n",
        "    # print(b_bins.shape)  # [1,1,313]\n",
        "    a_hat = (z_hat_softmax * a_bins).sum(2)\n",
        "    b_hat = (z_hat_softmax * b_bins).sum(2)\n",
        "    a_hat = a_hat.unsqueeze(0)\n",
        "    b_hat = b_hat.unsqueeze(0)\n",
        "    # print(a_hat.shape)  # [1, 64, 64]\n",
        "    ab_hat = torch.cat((a_hat, b_hat), 0)\n",
        "    # print(ab_hat.shape) # [2, 64, 64]\n",
        "    return ab_hat.permute(1,2,0)\n",
        "\n",
        "def f_t(z_hat):\n",
        "    \"\"\"\n",
        "    f_t function in equation 5 \n",
        "    input Z_hat has dimensions (1, 313, 64, 64)\n",
        "    ??? output f_t_Z dimension H * W * Q\n",
        "\n",
        "    - Each of the 313 Q probability values of each H * W pixels is converted to \n",
        "      softmax probabilites.\n",
        "\n",
        "    - The final value is the expected value of all these 313 softmax probablities across the color bins\n",
        "      this is calculated in the H function\n",
        "    \"\"\"\n",
        "    T = 0.38 # From page 6 of paper\n",
        "    z_hat_exp = torch.exp(torch.log(z_hat)/T) # dimension: H * W * Q\n",
        "\n",
        "    deno = torch.sum(z_hat_exp, dim = 1)\n",
        "    z_hat_softmax = z_hat_exp / deno \n",
        "    return z_hat_softmax\n",
        "\n",
        "# Test with random image from end of dataset\n",
        "from random import seed\n",
        "from random import randint\n",
        "\n",
        "# l, ab = dataset[randint(0, 100) * -1]\n",
        "# l, ab = testing_dataset[11] # minibatch 0\n",
        "l, ab = training_dataset[0] # minibatch 0\n",
        "l = l[0:1] # image 0\n",
        "ab = ab[0:1] # image 0\n",
        "z_hat = net(l)\n",
        "ab_hat = H(z_hat)\n",
        "ab_hat = ab_hat.permute(2,0,1).unsqueeze(0)\n",
        "ab_hat_256 = F.interpolate(ab_hat, (256, 256))\n",
        "img_rgb = reconstruct_predicted(l.cpu(), ab_hat_256.detach().cpu())\n",
        "plt.imshow(img_rgb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_vITHxmD7uS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: add v(.) coefficient to the loss function (color rarity rebalancing term)\n",
        "# TODO: optimal minibatch size? Passing minibatch of 32 images to the network - legit? faster?\n",
        "# TODO: batch norm in NN required?"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}